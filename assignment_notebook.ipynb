{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9457660a-2c95-4113-93eb-5054e40c0e01",
   "metadata": {},
   "source": [
    "# Datascience for Economics Final Assignment\n",
    "***\n",
    "|Name|SNR|ANR|\n",
    "|----|---|----|\n",
    "|Luka Parisi|2066677|u500977|\n",
    "|Atma Jyoti Mahapatra|2084556|u302557|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a888aff-4835-44c8-8287-d14ed875eb5e",
   "metadata": {},
   "source": [
    "***\n",
    "# A. Research question\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa23e1cc-0475-4b9d-967d-c90564dd0f6e",
   "metadata": {},
   "source": [
    "**This study will try to determine the relationship between poverty and crime in the US using county level data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee31f2-5108-4e4e-917b-1ac1aeeb2374",
   "metadata": {},
   "source": [
    "***\n",
    "# B. Motivation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a179d-8d03-4895-9fd5-4cff86148015",
   "metadata": {},
   "source": [
    "There have been a multitude of studies, which have tried to disentangle the determinants and causal mechanisms behind crime. Yet, the exact factors which drive crime, remain  quite ambiguous and are highly determined by the individual setting and a number of different factors. The economic theory of crime mostly focuses on factors such as poverty, education, family background, personal incentives, probability of arrest, race, sex, population density and other economic and social factors. Moreover, it is a key social concern to mitigate crime, and this thought is reflected by Aristotle, who succintly captured this by saying:\n",
    "\n",
    "> ***\"Poverty is the parent of crime and revolution\"***\n",
    "\n",
    "Given the relevance of this issue, it is pertinent that this be viewed through the lens of economics as well. Moreover, since we are both economists that are interested in poverty and inequality, we have choosen to analyze how poverty affects crime rates in the United States using county level data which was available for years 2010 and 2016. We explicitly chose to analyze data from the United States, since it has historically been troubled with poverty levels that are quite high for a developed and rich country (source: [OECD](https://data.oecd.org/inequality/poverty-rate.htm)). Furthermore, the United States has a number of cities that have gained international notoriety for high levels of crime (such as Baltimore, St Louis, Detroit, and others.) and the country itself has been plagued with unexplainable instances of violent crime making it interesting for research.     \n",
    "\n",
    "Our initial idea was to analyze poverty and crime through the lense of each state individually. Instead, we decided it would be smarter to conduct our analysis on the county level since states can vastly differ by their respective counties. Hence, we concluded that looking at aggregated state data would not be as informative as county level data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b4c956-330f-4779-873b-0c743e887695",
   "metadata": {},
   "source": [
    "***\n",
    "# C. Method and data\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f617d-5996-479b-98a5-01e625cd3ef9",
   "metadata": {},
   "source": [
    "For this analysis we decided to merge three datasets:\n",
    "1. County-Level Detailed Arrest and Offense Data, United States, 2010 (https://www.icpsr.umich.edu/web/NACJD/studies/33523#): \n",
    "    - This data collection contains county-level counts of arrests and offenses for most counties in the United States for the year 2010.\n",
    "2. County-Level Detailed Arrest and Offense Data, United States, 2016 (https://www.icpsr.umich.edu/web/NACJD/studies/37059#):\n",
    "    - This data collection contains county-level counts of arrests and offenses for most counties in the United States for the year 2016.\n",
    "    \n",
    "    (*We will explain the weaknesses of these two crime datasets in further detail below.*)\n",
    "3. County level economic and demographic data for the years 2010 and 2016 (https://www.openintro.org/data/?data=county_complete):\n",
    "    - This was one of a few datasets where a vast amount of information was available for each county in the United States. It was compiled from a multitude of different sources such as the Census Bureau, Bureau of Labor Statistics, etc.\n",
    "\n",
    "*For full transparency datasets with all accompanying folders have been provided in our github repository.*\n",
    "\n",
    "The county level economic and demographic dataset for years 2010 and 2016 had a vast number of different variables (182). For our analysis we only needed a certain number of variables which were present for both the year of 2010 and 2016. Variables included:\n",
    "- name of state\n",
    "- name of county\n",
    "- fips code (this is our ID variable)\n",
    "- population (variable needed to calculate arrest rate per 100k and population density)\n",
    "- % of population that has obtained high school degree\n",
    "- % of population below poverty level (our independent variable of interest)\n",
    "- % of population unemployed\n",
    "- median household income\n",
    "- area of county (variable needed to calculate population density)\n",
    "    \n",
    "In the county level arrest datasets, we only choose the total number of arrests within the county in a given year (2010 and 2016, we will transform this variable into our dependent variable of interest) and the county and state code (which will be our ID variable and help us merge the two datasets) since all other demographic and economic variables were available in the previously mentioned county level dataset. \n",
    "\n",
    "The first challenge, beyond just choosing variables of interest, was to merge the multiple datasets. Each county in the United States has a unique FIPS code which we used as an identifier. Unfortunately, the datasets had different ways of representing this code. In order to merge the three datasets, we created a new column in the two FBI datasets in which we imputed the correct FIPS codes so that the FIPS codes would match between all of the datasets (we also modified the FIPS codes in the county-level dataset since they were missing a 0 for all the counties from the first 10 states). \n",
    "\n",
    "Furthermore, the county-level dataset grouped all variables of interest for both years (2010 and 2016) in columns further complicating the cleaning and merging. We decided that it would be easiest to compile two dataframes, county-level data merged with arrest statistics for the year 2010 and the same dataframe for the year of 2016. Later, we concatenated these two dataframes since they had identical columns, sorted them and set the respective indexes. Finally, we created a few variables such as arrest rate per 100k and population density through calculation of already available variables and finished cleaning the data.  \n",
    "\n",
    "After merging the data, we proceeded to deal with missing values and outliers (IQR). Through graphical exploration of our clean data, we could see that there was some form a linear relationship between our independent variable of interest - `poverty rate` and our dependent variable of interest - `arrest rate per 100k`. It suggested that higher poverty rates were correlated with higher rates of arrests. It is important to note that our dependent variable `arrest rate per 100k` was used as a proxy for crime rate (we will assume that arrest rates are a good representation of crime rates). In the next step we ran a linear regression using an ordinary least squared method. In order to confirm our findings we also compared it with the results obtained by a Bayesian regression model. Before running our bayesian regression, we standardized all of our variables of interest as to help the algorithm work. To wrap up our work, we evaluated the obtained results by interpreting the posterior distributions and traceplots and concluded on our findings. \n",
    "\n",
    "We specified the following model to test our hypothesis:\n",
    "\\begin{equation}\n",
    "\\label{eq:1}\n",
    "Arrest\\ per\\ 100k = \\beta_0 + \\beta_1 Poverty\\ rate + \\beta_2 \\ln(Population\\ density)+ \\beta_3 Unemployment\\ rate\n",
    "\\end{equation}\n",
    "\n",
    "As noted in the introduction, besides the `poverty rate`, there are a multitude of factors that have been identified as determinants of crime. We choose to add population density as a control, since it has been cited that in areas with higher population density crime increases. We used the log of this variable since it had extreme values and was quite skewed. Furthermore, we also added the unemployment rate, hypothesizing that higher unemployment rates would increase crime rates in counties. Previous graphical exploration of our control variables could show that the relationships were in the direction of our intuiton. We also wanted to include variables such as education and race, but they were highly correlated with other independent variables such as the `poverty rate`. Hence, we choose to exclude these variables, since multicollinearity would make our results harder to interpret.  \n",
    "\n",
    "We believe that our data is representative to the underlying population since it contained data from a large number of counties. Obviously, data collection could have been more frequent which would have enabled us to pinpoint changes in our key variables with more precision. Also, the crime data was collected by a number of different agencies in different counties making this process challenging and prone to error (as noted in the dataset documentation). In the  `Main assumptions` chapter we will further clarify certain problems with our approach that could have been solved if more variables could have been available. Furthermore, we will also explain the exact problems of the crime datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9744f8a-5b5f-425d-bdae-947e0a35126d",
   "metadata": {},
   "source": [
    "***\n",
    "# D. Preview of the answers\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b54f85-07c6-427c-a046-453940e38f32",
   "metadata": {},
   "source": [
    "In line with our hypothesis, we find that there is a positive relation between poverty and crime. These findings are confirmed by both OLS and Bayesian regressions. In our bayesian analysis, the density plots for different chains were very similar for each parameter which indicates that they were sampled from the right posterior. Furthermore, all R hat values had a value of 1, which confirms that all the chains converged successfuly. The coefficient of interest indicates that a 1 standard deviation increase in poverty would increase crime rates by 0.16 standard deviations. Finally, we also tested the fit of the model and we could see that most of the observations were within the 95% confidence intervals, which means the model fit the data well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df853354-28c4-46e8-82f1-34b41ac9adf7",
   "metadata": {},
   "source": [
    "***\n",
    "# E. Main assumptions\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60843b-baa1-4909-89ae-9e808026b52a",
   "metadata": {},
   "source": [
    "One of the main assumptions we made was that arrest rates are a good representation of crime rates (since we used arrest rates as our proxy variable for crime rates). Even with this assumption, we cannot interpret our findings as causal, since we believe there was a confounded relationship between poverty and arrest/crime rates. We were unable to control for probability of arrest/number of police within counties. This means that our results are probably biased and that the effect of poverty is smaller/larger than in the \"real world\". Furthermore, a multicollinearity between certain variables such as high school education and poverty, demanded us to exempt these variables from our model, thus not allowing us to control for them.\n",
    "\n",
    "Knowing this beforehand, we obviously wanted to approach this problem through a methodology where we could isolate exogenous variation in poverty rates. Unfortunately for us, poverty is not randomly assigned by birth and any experiment in this field would be deemed highly unethical, meaning that data that would help us analyze this problem casually was unavailable. Finally, we could not find a valid instrument since there are not enough variables at the county-level which would be highly correlated with poverty rates, and not affect crime rates directly.\n",
    "\n",
    "Additionally, the crime datasets were missing all observations for two states: Florida and Illinois. Also, this data was collected individually by a large number of different agencies within different states and counties, making it prone to errors. However, it was harmonized by the FBI, which allows us to place some degree of trust in the dataset.\n",
    "\n",
    "We decided to work on this topic regardless of the aforementioned problems. The main reason was that we found this topic extraordinarly interesting, and that even with these problems, we believe that our findings still have value, even if they cannot be causally interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d89dee-c06c-4c9f-b4cd-3eb319692e3a",
   "metadata": {},
   "source": [
    "***\n",
    "# F. Python/R code\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae0b649-ffa9-4f9c-bb4e-940ff0279b5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pips necessary for analysis \n",
    "!pip install missingno\n",
    "!pip install \"plotly>=5\" \"ipywidgets>=7.6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4304029c-d607-4c0f-bd21-cc407859590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary packages\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import arviz as az\n",
    "import plotly\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "        counties = json.load(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123765b4-5b88-4458-b24c-61808cdc9c01",
   "metadata": {},
   "source": [
    "***\n",
    "## F.1. Loading, merging and cleaning datasets\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad88f3-f4a5-48b4-ae3f-2bc1c4e5a409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading county level data \n",
    "county = pd.read_csv('./Data/County_data/county_complete(1).csv')\n",
    "county.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae831da5-3365-4c76-b521-fb69110a189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting county level data for 2010 and choosing variables mentioned in the introduction; adding a year column for each dataset\n",
    "county2010 = county.loc[:,['state','name','fips',\n",
    "                           'pop2010','hs_grad_2010','poverty_2010','unemployment_rate_2010','median_household_income_2010','area_2010']]\n",
    "county2010['year'] = 2010\n",
    "#Subsetting county level data for 2016 and choosing variables mentioned in the introduction; adding a year column for each dataset\n",
    "county2016 = county.loc[:,['state','name','fips',\n",
    "                           'pop2016','hs_grad_2016','poverty_2016','unemployment_rate_2016','median_household_income_2016','area_2010']]\n",
    "county2016['year'] = 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa8c22-c58b-402b-88b9-fbf8a3845bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading crime data for 2010\n",
    "crime2010 = pd.read_stata('./Data/FBI_crime_data/ICPSR_33523/DS0001/33523-0001-Data.dta')\n",
    "#Subsetting crime data for 2010 and using str.zfill to make FIPS code mergable with county dataset (2010)\n",
    "crime2010 = crime2010.loc[:,['FIPS_ST','FIPS_CTY','COVIND','GRNDTOT']]\n",
    "crime2010['FIPS_ST'] = crime2010['FIPS_ST'].astype(str)\n",
    "crime2010['FIPS_ST'] = crime2010['FIPS_ST'].str.zfill(2)\n",
    "crime2010['FIPS_CTY'] = crime2010['FIPS_CTY'].astype(str)\n",
    "crime2010['FIPS_CTY'] = crime2010['FIPS_CTY'].str.zfill(3)\n",
    "crime2010[\"fips\"] = crime2010[\"FIPS_ST\"] + crime2010[\"FIPS_CTY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae5aa9-ecd4-449e-8d4e-1350528aee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading crime data for 2016\n",
    "crime2016 = pd.read_stata('./Data/FBI_crime_data/ICPSR_37059/DS0001/37059-0001-Data.dta')\n",
    "#Subsetting crime data for 2016 and using str.zfill to make FIPS code mergable with county dataset (2016)\n",
    "crime2016 = crime2016.loc[:,['FIPS_ST','FIPS_CTY','COVIND','GRNDTOT']]\n",
    "crime2016['FIPS_ST'] = crime2016['FIPS_ST'].astype(str)\n",
    "crime2016['FIPS_ST'] = crime2016['FIPS_ST'].str.zfill(2)\n",
    "crime2016['FIPS_CTY'] = crime2016['FIPS_CTY'].astype(str)\n",
    "crime2016['FIPS_CTY'] = crime2016['FIPS_CTY'].str.zfill(3)\n",
    "crime2016[\"fips\"] = crime2016[\"FIPS_ST\"] + crime2016[\"FIPS_CTY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e36a152-b26b-4ea4-985d-7177bc044858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finishing up cleaning fips codes in the county datasets such that the fips codes match for each county between all the datasets\n",
    "county2010['fips'] = county2010['fips'].astype(str)\n",
    "county2010['fips'] = county2010['fips'].str.zfill(5)\n",
    "county2016['fips'] = county2016['fips'].astype(str)\n",
    "county2016['fips'] = county2016['fips'].str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b0cf2a-88bf-45dc-a7e9-249e1157d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging dataframes for years 2010 and 2016 using .merge\n",
    "county2010 = county2010.merge(crime2010,how='left',left_on=['fips'], right_on=['fips'])\n",
    "county2016 = county2016.merge(crime2016,how='left',left_on=['fips'], right_on=['fips'])\n",
    "#Renaming columns such that they have the same names and can be merged together using pd.concat\n",
    "county2010.rename({'pop2010':'population','hs_grad_2010':'hs_grad','poverty_2010':'poverty', 'unemployment_rate_2010':'unemployment_rate',\n",
    "                          'median_household_income_2010':'median_hh_income'},inplace=True,axis=1)\n",
    "county2016.rename({'pop2016':'population','hs_grad_2016':'hs_grad','poverty_2016':'poverty', 'unemployment_rate_2016':'unemployment_rate',\n",
    "                          'median_household_income_2016':'median_hh_income'},inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de392a9-17fd-40c7-b2bb-89ab3fe86a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking result of merge between crime and county data for year 2010\n",
    "county2010.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09469e-1364-43b7-a5c2-3e6c768275cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking result of merge between crime and county data for year 2010\n",
    "county2016.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054e99c-bfcb-469f-843d-18f817cbcba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging dataframes for 2010 and 2016 and dropping columns that we dont need anymore; finally checking result of concat\n",
    "frames = [county2010,county2016]\n",
    "county_crime = pd.concat(frames)\n",
    "county_crime = county_crime.reset_index()\n",
    "county_crime.drop(['FIPS_ST','FIPS_CTY','index'],inplace = True,axis=1)\n",
    "county_crime.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08fd254-277d-4046-8066-ab1ae93de055",
   "metadata": {},
   "source": [
    "***\n",
    "### F.1.1 Creating variables of interest that are needed for further analysis:\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c33271-87ad-4539-b442-3f816d86b0dd",
   "metadata": {},
   "source": [
    "**Arrest rate** – An arrest rate describes the number of arrests made by law enforcement agencies per 100,000 people. We calculate arrest rate by dividing the number of reported arrests by the county population. The result is multiplied by 100,000. We will use this variable as our dependent variable and proxy variable for crime rate per 100k.\n",
    "\n",
    "**Natural logarithm of population density** - Population density = County population / Land area. After this we take the natural logorithm of population density, since the variable has extreme values and it will be easier to work in the logarithmic form.\n",
    "\n",
    "Furthermore, we create a variable which we will use in the next chapter to create a US county map for arrest rates. We will delete this variable once we are done with the introductory graphical exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c3218-525d-4d8b-8118-d3e76daa325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculations  \n",
    "county_crime['arrest_rate'] =  (county_crime.GRNDTOT/county_crime.population)*100000\n",
    "county_crime['l_population_density'] = np.log(county_crime.population/county_crime.area_2010)\n",
    "county_crime['arrest_rate_map'] = (county_crime.GRNDTOT/county_crime.population)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6418dfec-9a73-4e61-acff-bcf650edccf5",
   "metadata": {},
   "source": [
    "We still have some data cleaning left, such as setting indexes and further removing variables. We did not finish all of the data cleaning in this chapter because some of these variables were necessary to create graphs and deal with missing data in the correct way (as you will see in the next chapter). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e738cf22-62c1-48e7-97bb-05a77708fb7f",
   "metadata": {},
   "source": [
    "***\n",
    "## F.2. Introductory graphical exploration\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551fe643-0da4-45ca-9231-6ee570a0e3d8",
   "metadata": {},
   "source": [
    "### F.2.1 Overview of crime rate in different US counties in 2010 and 2016, using a chloropleth map\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b0cd1-f9ea-4d41-abb8-0800b60743a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#american map - figure\n",
    "fig = px.choropleth(county_crime,\n",
    "                    geojson = counties,\n",
    "                    locations='fips',\n",
    "                    color='arrest_rate_map',\n",
    "                    color_continuous_scale=\"Viridis\",\n",
    "                    range_color=(0, 6),\n",
    "                    scope=\"usa\",\n",
    "                    animation_frame=\"year\",\n",
    "                    labels={'arrest_rate_map':'Arrest rate'},\n",
    "                    width = 800,\n",
    "                    height = 600,\n",
    "                    title = ' Figure 1 - Change in arrest rate in US counties'\n",
    "                    )\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb099e16-a306-4759-a207-f1390e8379a1",
   "metadata": {},
   "source": [
    "Looking at the US county map, we can first notice that two entire states - Florida and Illinois are filled with zeroes (0). Other than that we do not really notice a clear trend between the years except that it seems like arrest rates fall slightly in 2016, compared to 2010. In the next chapter, we will try to deal with zeroes that seem to distinguish actual missing data rather than actual zeroes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc18957-c0cf-4163-9f84-558649e726dd",
   "metadata": {},
   "source": [
    "***\n",
    "### F.2.2 Checking the distribution of variables in the dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e445a51-8dc5-4d91-9706-56716265b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking distribtuion of variables\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "sns.histplot(data=county_crime, x=\"arrest_rate\", kde=True, color=\"skyblue\", ax=axs[0, 0]).set(title='Distribution of arrest_rate')\n",
    "sns.histplot(data=county_crime, x=\"poverty\", kde=True, color=\"olive\", ax=axs[0, 1]).set(title='Distribution of poverty')\n",
    "sns.histplot(data=county_crime, x=\"l_population_density\", kde=True, color=\"gold\", ax=axs[1, 0]).set(title='Distribution of l_population_density')\n",
    "sns.histplot(data=county_crime, x=\"unemployment_rate\", kde=True, color=\"teal\", ax=axs[1, 1]).set(title='Distribution of unemployment_rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5990e7e-0714-42ab-bad5-feb1848c905c",
   "metadata": {},
   "source": [
    "As we can see from the distribtuion of our variable of interest - `arrest_rate`, there are a large number of zeroes in our dataset and a few large outliers from 10.000 up to 60.000. We will deal with this problem in the following chapters. This large number of observed zeroes is a fault of this dataset which contains true zeroes and zeroes that constitute missing data (instead of NaN's the original dataset has inputed 0 for missing observations).\n",
    "\n",
    "The other variables seem to be less skewed than the `arrest_rate`, which inspires confidence that the main issue to deal with is the skewness of `arrest_rate`.\n",
    "\n",
    "Other than that, `arrest_rate` and `poverty` is distributed in a similar manner between 2010 and 2016, as shown by the following two interactive histograms: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a080c-d44e-4816-91a1-85fc8b1f9d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Looking at distribution of dependent variable through 2010 and 2016\n",
    "fig = px.histogram(county_crime, x=\"arrest_rate\", color=\"year\", width = 500, height = 400, title='Distribution of arrest_rate between 2010 and 2016')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33421e4-f78c-45bf-ae1d-0308f815f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(county_crime, width = 500, height = 400, x=\"poverty\", color=\"year\", title = \"Distribution of poverty between 2010 and 2016\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f1d7a-8799-467d-a26b-54a18e3be9cf",
   "metadata": {},
   "source": [
    "Our independent variable of interest `poverty` seems to have a more normal-like distribution. Most of the values are centered around 10% to 20%, with a few small and large values. This variable is also distributed roughly similarly between the two years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d6630f-4a5c-4109-8f1f-3d7c7209ba3e",
   "metadata": {},
   "source": [
    "***\n",
    "## F.3. Dealing with missing values\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c494162-995e-4e3e-ad60-9638b3265b8f",
   "metadata": {},
   "source": [
    "As noted above,  we have a lot of zeroes (0) for our `arrest_rate` variable. This is large part due to the fact that missing data is recorded as 0. We now delve deeper into this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda73cca-5674-43eb-9b6f-84afdfa5736a",
   "metadata": {},
   "source": [
    "### F.3.1 Counting missing values in data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb74330-5fd6-47d0-bf85-70da8ab501ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final sorting of dataset and setting indexes for next chapters so that we can deal with outliers and missing values; checking final result\n",
    "county_crime.drop(['area_2010','arrest_rate_map'],inplace=True,axis=1)\n",
    "county_crime = county_crime.sort_values(['fips','year'])\n",
    "county_crime.set_index(['fips','name','year'],inplace=True)\n",
    "county_crime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b56c7-dd9a-44d1-9bc9-8ffb16c143b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function which can analyze the dataset in terms of missing values and type of columns\n",
    "def Analysis(data):\n",
    "    print(\"Analysing\")\n",
    "    print(data.info())\n",
    "    #Calculating the share of missing values\n",
    "    missing = pd.concat([data.isnull().sum(), 100 * data.isnull().mean()], axis=1)\n",
    "    missing.columns=['Count', '%']\n",
    "    missing = missing.sort_values(by='Count', ascending=False)\n",
    "    print(missing)\n",
    "    df=missing.iloc[0:5,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3028bc-7e49-41fa-93eb-b37d7d2474bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Analysis(county_crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8d6953-bb9b-4dd7-ad1e-bb919ea1413c",
   "metadata": {},
   "source": [
    "Our \"final\" dataset has only a small amount of missing values. But the bigger problem lies within the true and false zeroes, as we saw during graphical exploration. As noted in the data documentation \"In this data collection, zeroes may represent true zeroes or missing data, and it is possible to distinguish between the two\". The following image shows the difference between a true zero and a zero that represents missing data:\n",
    "![Methodology for missing data](Missing_data.PNG)\n",
    "\n",
    "Hence, zeroes that are not true zeroes have a coverage indicator of 0 and all arest count variables that are equal to 0 (basically our key variable equal to 0, because it is built on the sum of all arrest count variables from the original dataset). To find these true zeroes, we will use the code in the following cell, and transform the arrest rate to NaN if it follows the condition from the above attached image (Coverage indicator = 0 AND Arrest rate = 0).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ede59c-928d-4eb3-bc82-4140fee44169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = ['arrest_rate', 'COVIND', 'GRNDTOT'] # identifies columns relevant for identifying missing values\n",
    "county_crime[cols] = county_crime[cols].mask(county_crime[cols].eq(0).all(axis=1)) # creates a masked array if all three columns mentioned in \"cols\" is 0\n",
    "#Drop all variables we dont need anymore\n",
    "county_crime.drop(['COVIND','GRNDTOT'],inplace=True,axis=1)\n",
    "#Create matrix that shows us the position of missing values \n",
    "msno.matrix(county_crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea456c-726f-4f2a-b470-55570c9610a8",
   "metadata": {},
   "source": [
    "As we can see from the missing observation matrix, most of the missing values are connected meaning that they come from the same states - Florida and Illinois. In the next fragment of code we will see the exact number of missing observations and we will discuss our way of dealing with this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eccc30-6cfb-4751-aa12-f54275133579",
   "metadata": {},
   "outputs": [],
   "source": [
    "Analysis(county_crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beeea3b-d39f-4988-b819-9615203693e0",
   "metadata": {},
   "source": [
    "***\n",
    "### F.3.2 Dealing with missing variables\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b91e2-78fa-42a5-9b7a-e0c59c852ef2",
   "metadata": {},
   "source": [
    "There is a total of 362 missing values for arrest rates (5.76% of our dataset). In our work, we will try two approaches for dealing with missing data:\n",
    "\n",
    "1. We will **drop** the **missing variables** (complete case analysis - because only records that are complete get retained for the analysis)\n",
    "2. We will use IterativeImputer or popularly called **MICE** (Multiple Imputation by Chained Equation) for imputing missing values (the IterativeImputer performs multiple regressions on random samples of the data and aggregates for imputing the missing values)\n",
    "\n",
    "MICE operates under the assumption that the observations are missing at random (MAR), which implies that the missingness of a field can be explained by the values in other columns, but not from that column. Regardless of the existing limitations of our data, we believe that other county level variables such as population, poverty rates, etc. can reasonably explain arrest rates within a given county.\n",
    "Moreover, KNN (K Nearest Neighbors) method of imputation does not translate well here. This is because our missing values are clustered together, and they are from the same states, so this method lacks data to impute from, at its very base.\n",
    "Lastly, dropping variables (complete case analysis) is the easiest approach, and it gives us a complete dataset, hence this is worth exploring. However, it drops more than 5% of our dataset, which is something that should be considered, since we can use the information from all the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112aff2-0152-4a56-806f-7244b80d6b32",
   "metadata": {},
   "source": [
    "#### F.3.2.1 Dropping variables\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a3c16-1e10-4f17-bad9-bf7335706cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Dropping all missing values \n",
    "county_crime_dropna = county_crime.dropna()\n",
    "county_crime_dropna.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1827c2d3-abd6-4548-9b73-d37b8594c649",
   "metadata": {},
   "source": [
    "#### F.3.2.2 MICE imputation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82714e08-b411-446d-a246-8c957075d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. MICE imputation\n",
    "# Initializing the imputer\n",
    "MICE_imputer = IterativeImputer(skip_complete=True)\n",
    "county_crime_mice = county_crime.iloc[:,1:]\n",
    "# Imputing the missing observations\n",
    "county_crime_mice.iloc[:, :] = MICE_imputer.fit_transform(county_crime_mice)\n",
    "county_crime_mice.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac8970-4993-4fb9-9454-52092aaccac8",
   "metadata": {},
   "source": [
    "We can see that there are no large differences between the two datasets, besides the number of observations (MICE dataset has 363 more observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d110063f-6931-4aa1-bc42-f5817493d6e0",
   "metadata": {},
   "source": [
    "***\n",
    "### F.3.3 Deciding on method to deal with missing dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660d13e-c1c0-4aef-b18d-7e09fe8f89df",
   "metadata": {},
   "source": [
    "When deciding which dataset to use, we choose to maximize the information we can glean from our dataset, and MICE allows us to do that. Comparing the summary statistics from both datasets, we can see that they are rather similar, and choosing either one would be fine to work with. However, with MICE imputation, we have 363 more rows of data (more than 5% of our dataset), which provides a richer dataset for analysis, by retaining information from the other variables.\n",
    "\n",
    "Hence, moving forward, we will only be working with the MICE dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac13c11-f81f-4dc4-b012-4bb133a1b37f",
   "metadata": {},
   "source": [
    "***\n",
    "## F.4. Detecting outliers in our data\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba20c82-0df3-495b-8186-56623a14875a",
   "metadata": {},
   "source": [
    "In the previous chapter we plotted the distribution of arrest rate and we saw that it has extreme values in the right tail. Furthermore, we can see that the maximum arrest rate and standard deviation for arrest rate is quite high. Since our goal is Bayesian regression analysis, we will exclude some of the largest outliers, such that our results do not reflect results that are solely created by these large outliers, resulting in a better fit for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6743b-df9d-4236-a01f-c76d111ccf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring highest values of arrest rate per 100k\n",
    "print(county_crime_mice['arrest_rate'].nlargest(n=10))\n",
    "print(county_crime_mice['arrest_rate'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59efe161-f37a-47e8-8441-11e251cac7ff",
   "metadata": {},
   "source": [
    "Arrest rates as high as these would mean that 63,221 out of 100,000 people have been arrested within a given county. These are extreme values which probably do not accurately reflect the crime in a given county. Furthermore, we can see that the variable `arrest_rate` has a skew of 4.86, which is quite large. \n",
    "\n",
    "To detect and clean outliers from our data we will use the interquartile range (IQR). IQR is a measure of statistical dispersion and is calculated as the difference between the 75th and 25th percentiles. It is represented by the formula IQR = Q3 − Q1. The lines of code below create a function that calculates and print the interquartile range for our final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de43f3-371b-494b-8873-d9a4caec93fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outliers = []\n",
    "def detect_outliers_iqr(data):\n",
    "    data = sorted(data)\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    # print(q1, q3)\n",
    "    IQR = q3-q1\n",
    "    lwr_bound = q1-(1.5*IQR)\n",
    "    upr_bound = q3+(1.5*IQR)\n",
    "    # print(lwr_bound, upr_bound)\n",
    "    for i in data: \n",
    "        if (i<lwr_bound or i>upr_bound):\n",
    "            outliers.append(i)\n",
    "    return outliers# Driver code\n",
    "sample_outliers_mice = detect_outliers_iqr(county_crime_mice['arrest_rate'])\n",
    "print(sample_outliers_mice)\n",
    "print(len(sample_outliers_mice))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440767b-c608-435a-b6a6-57edfd4f1b07",
   "metadata": {},
   "source": [
    "As we can see, the mice dataset has 184 outliers. All of the outliers are on the right side of the distribution. Hence, we will remove all of these outliers in the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cabcb54-9171-46d8-bf8e-1aa8b45703e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop outliers, we are using min(sample_outliers) because we only have outliers on the right of our distribution\n",
    "county_crime_mice = county_crime_mice.loc[county_crime_mice['arrest_rate'] < min(sample_outliers_mice)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0673c13-2dec-4863-8500-8322657fcb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "sns.histplot(data=county_crime_mice, x=\"arrest_rate\", kde=True, color=\"navy\", ax=axs[0, 0]).set(title='Distribution of arrest rate (MICE Imputation)')\n",
    "sns.histplot(data=county_crime_mice, x=\"poverty\", kde=True, color=\"olive\", ax=axs[0, 1]).set(title='Distribution of poverty rate (MICE Imputation)')\n",
    "sns.histplot(data=county_crime, x=\"arrest_rate\", kde=True, color=\"navy\", ax=axs[1, 0]).set(title='Distribution of arrest rate (Baseline)')\n",
    "sns.histplot(data=county_crime, x=\"poverty\", kde=True, color=\"olive\", ax=axs[1, 1]).set(title='Distribution of poverty rate (Baseline)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d68b1e8-de19-4f84-b2ba-09ba4435e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(county_crime_mice['arrest_rate'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d4641-181d-4295-8b59-9504eaf0ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(county_crime['arrest_rate'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a860ca-5adb-45a9-b60c-70b956382682",
   "metadata": {},
   "source": [
    "These results show that we have successfuly dealt with the outliers in the data, and the data is not as skewed as it was in the original dataset. The variable of interest `arrest_rate` has a much less skewed distribution, while other variables such as `poverty` are not affected by this method. Finally we check the skew index of `arrest_rate` and we can see that it reduces drastically from 4.74 to 0.37, indicating that the method effectively removed outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2881f791-5487-44c3-a115-b0fb5c8b7cb5",
   "metadata": {},
   "source": [
    "***\n",
    "## F.5. Linear regression\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0854df-fc10-4273-8074-7997f8f4a79b",
   "metadata": {},
   "source": [
    "### F.5.1 Relationship between `arrest_rate` and other variables\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326cdfbf-fcb1-4b36-85df-2c730e3ee7ba",
   "metadata": {},
   "source": [
    "Here we lay our foundations for how we expect `arrest_rate` to move, with other variables. This is based on economic theory, and then we present graphs based on the data, that test our hypothesis. \n",
    "\n",
    "* We can expect `arrest_rate` to **increase** as `poverty` increases. If people are under the poverty level, they turn to crime to try to sustain their lives and consequentially, arrest rates are higher.\n",
    "\n",
    "* We can expect `arrest_rate` to **increase** as `unemployment_rate` increases. Similar to `poverty`, `unemployment` also can lead people to a criminal lifestyle. However, the important difference is that unemployed people are usually better educated and by definition, are still part of the labor force. They have an incentive to rejoin the labor force, and earn a sustenance.\n",
    "\n",
    "* We can expect `arrest_rate` to **decrease** as `hs_grad` increases. As more people finish high school and get an education, they move into the labor force and usually stay out of crimes for which they can be arrest.\n",
    "\n",
    "* We can expect `arrest_rate` to **increase** as `l_population_density` increases. An increase in population can mean a number of different things. For example, individuals that live in more densely populated areas mostly live in cities. Cities have larger populations and individuals have more opportunities to commit crime. Furthermore, in small rural areas, communities are connected and commiting crime is frowned upon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadaa14e-18f3-41c1-8784-2067fbcd3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "sns.regplot(x=county_crime_mice['poverty'], y=county_crime_mice['arrest_rate'], ax=axs[0, 0], scatter_kws={'alpha':0.1}, line_kws={\"color\": \"red\"}).set(title='Poverty and arrest rate')\n",
    "sns.regplot(x=county_crime_mice['unemployment_rate'], y=county_crime_mice['arrest_rate'], ax=axs[0, 1], scatter_kws={'alpha':0.1}, line_kws={\"color\": \"red\"}).set(title='Unemployment and arrest rate')\n",
    "sns.regplot(x=county_crime_mice['hs_grad'], y=county_crime_mice['arrest_rate'], ax=axs[1, 0], scatter_kws={'alpha':0.1}, line_kws={\"color\": \"red\"}).set(title='Education and arrest rate')\n",
    "sns.regplot(x=county_crime_mice['l_population_density'], y=county_crime_mice['arrest_rate'], ax=axs[1, 1], scatter_kws={'alpha':0.1}, line_kws={\"color\": \"red\"}).set(title='Population density and arrest rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d61fd7-f4b2-48c2-b2e9-44664c4db7bd",
   "metadata": {},
   "source": [
    "As we can see from the four plots above, even though most of the observations are stacked together all of the relationships are as we anticipated. With an increase in poverty rates/population density/unemployment rate, it seems like arrest rates also increase slightly, and the opposite holds true for education and arrest rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157632a-b4b2-44ea-9c65-9cf78c2303ce",
   "metadata": {},
   "source": [
    "***\n",
    "### F.5.2 Correlation heatmap and specifying our regression model\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93a0ab-a73a-4fbc-82d7-7c106b07e738",
   "metadata": {},
   "source": [
    "Having laid down the groundwork for the intuitive reasoning behind our dependent and independent variables, and having cross-checked and validated this pattern in the data, we now move on to the analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e471b10-cabb-45e6-9164-20aa23017879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction a correlation Heatmap\n",
    "corrMatrix = county_crime_mice.corr()\n",
    "#makes a correlation matrix using the relevant variables\n",
    "fig8, ax = plt.subplots(figsize=(9,7.5))\n",
    "ax.set_title('Correlation Matrix',fontweight=\"bold\")\n",
    "sns.heatmap(corrMatrix, annot=True, linewidths=0.75,cmap=\"YlGnBu\");\n",
    "# sns.heatmap plots a correlation heatmap\n",
    "# annot shows the corrleation value\n",
    "# line width just to have some space between squares\n",
    "# cmap to choose a different colour scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43c35d8-9968-444b-8b26-07b53aada266",
   "metadata": {},
   "source": [
    "We can see that our independent variable poverty is highly correlated with hs_grad (% of population that has obtained high school degree) and median_hh_income (median household income). Unfortunately, because of this we will exclude hs_grad from the analysis even though we still believe that education is an important determinant of crime. We will now specify our regression model:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:2}\n",
    "Arrest\\ per\\ 100k = \\beta_0 + \\beta_1 Poverty\\ rate + \\beta_2 \\ln(Population\\ density)+ \\beta_3 Unemployment\\ rate\n",
    "\\end{equation}\n",
    "\n",
    "Interestingly, poverty and unemployment do not seem to have a very high correlation. The official definitions (sourced from the Bureau of Labor Statistics) are:\n",
    "\n",
    "* **Unemployed**: People who are jobless, looking for a job, and available for work are unemployed.\n",
    "\n",
    "* **Poverty**: If a family's total income is less than the family's threshold, then that family and every individual in it is considered in poverty.\n",
    "\n",
    "This implies that while people in poverty can work and still earn less than the threshold, unemployed people by definition do not work, and are actively seeking work. While it may be difficult for the former group to exit the *poverty trap*, it is also important to account for the latter, so that we can understand how economic fluctuations can also affect crime. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21949c7-248f-4a9d-84b5-28d2f1b32e09",
   "metadata": {},
   "source": [
    "***\n",
    "### F.5.3 Running our linear regression model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff92a4f1-4499-45e4-8b0d-0a5f730c42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying regression\n",
    "X = sm.add_constant(county_crime_mice[['poverty','l_population_density','unemployment_rate']])\n",
    "y = county_crime_mice['arrest_rate']\n",
    "lm = sm.OLS(y, X).fit()\n",
    "print('\\nSummary_mice: ', lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9770168-8764-48f0-9712-56103f5a7556",
   "metadata": {},
   "source": [
    "The frequentist conclusion from our regression output would be that given our data, a 1 unit increase in poverty rate would increase the number of arrests per 100k by 42. Our control variables also have positive coefficients as hypothesized, meaning with a 1 unit increase in population density/unemployment rate, arrest rates would also increase. All of the variables are statstically significant given p-values of 0.00. Obviously, our large sample of 6100 observations would probably mean that inserting any of our even remotely related variables would yield statistically significant findings (if variables would not be highly correlated between each other). Our $R^2$ is 0.109, meaning that about 11% of the overall variation in our dependent variable is explained by our independent variables. Even though $R^2$ is not very important for causal inference and economic models, it still feels like there is a lot of information we are missing when trying to explain crime rates. Our next step will be to run our Bayesian regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ca375-6ced-47e4-bddc-0a13252f6762",
   "metadata": {},
   "source": [
    "***\n",
    "## F.6. Bayesian analysis\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3bf36e-f2a7-4442-8328-e65caaa5105c",
   "metadata": {},
   "source": [
    "### F.6.1 An introduction to Bayesian analysis\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c1fd41-6f53-4b14-86f3-b7ec5e09140f",
   "metadata": {},
   "source": [
    "**Bayesian inference** incorporates prior beliefs about the systems we aim to model. These beliefs are combined with data to constrain the details of the model. Then, when used to understand a certain prediction/effect, the model doesn’t give one answer, but rather a distribution of likely answers, allowing a richer spread of information and further possibilities.\n",
    "\n",
    "Bayesian inference has long been a method of choice in academic science for just those reasons: it natively incorporates the idea of confidence, it performs well with sparse data, and the model and results are highly interpretable and easy to understand. Until recently the practical engineering challenges of implementing these systems were prohibitive, and required a large amount of specialized knowledge. However, this has become more mainstream, and many more software applications allow for Bayesian computation.\n",
    "\n",
    "Lastly, this technique also helps in offering a \"smoother\"/continuous way to deal with under/over-fitting, which consequentially is easier to interpret than lasso/ridge regressions.\n",
    "\n",
    "As a general statement, we can state Bayes theorem as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "P(A|B) = \\frac{P(A \\text{ and } B)}{P(B)} = \\frac{P(B|A) P(A)}{P(B)}\n",
    "\\end{equation}\n",
    "\n",
    "When we estimate a model, this takes the form of a posterior distribution for a parameter (vector of parameters) $\\theta$:\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\theta|\\text{data}) = \\frac{P(\\text{data}|\\theta) p(\\theta)}{P(\\text{data})}\n",
    "\\end{equation}\n",
    "\n",
    "The equation expresses how our belief about the value of \\\\(\\theta\\\\), as expressed by the *prior distribution* \\\\(P(\\theta)\\\\) is reallocated following the observation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe329088-f007-4b09-83f3-62a7dffb2a66",
   "metadata": {},
   "source": [
    "***\n",
    "## F.6.2 Standardizing our variables\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ffdc2-23ee-487e-9079-7d74559e4a89",
   "metadata": {},
   "source": [
    "In order to help the algorithm, we will first standardize our variables of interest. We have used the same standardization method as in the notebook -\n",
    "\n",
    "Standardization:\n",
    "\\begin{equation}\n",
    "\\label{eq:3}\n",
    "x_{standardized} = (x - mean(x)) / std(x)\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b433aea-88be-4e19-b36e-e7e6befb05e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    return (x-x.mean())/x.std()\n",
    "arrest_rate_norm = standardize(county_crime_mice['arrest_rate'])\n",
    "poverty_norm = standardize(county_crime_mice['poverty'])\n",
    "l_population_density_norm = standardize(county_crime_mice['l_population_density'])\n",
    "unemployment_rate_norm = standardize(county_crime_mice['unemployment_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952a3c58-695f-4972-95d2-fa03f04a86b4",
   "metadata": {},
   "source": [
    "***\n",
    "## F.6.3 Generating Bayesian model and trace plots\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473c900-a44e-41b9-9728-2bbaa6c0ffd0",
   "metadata": {},
   "source": [
    "After standardization we will complete the specification of our bayesian model. As in the notebook, we choose a small value for our $\\sigma_{prior}$, all in the purpose of avoiding overfitting. We will tune the algorithm with 750 samples and sample 1500 posterior draws overall. We have 5 parameters - the intercept, poverty rate, $\\ln$ of population density, unemployment rate and the standard deviation. When specifying our model, we will assume that arrest rate per 100k is normal because it is a sum variable and we dealt with outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538f80f-001c-4f1e-83b8-33d7b3c77857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formulating bayesian model\n",
    "#assuming arrest rate per 100k is normal because it is a sum variable and we dealt with outliers\n",
    "with pm.Model() as normal:\n",
    "    constant = pm.Normal('constant', mu = 0.0, sd = 1.0)\n",
    "    σ_prior = 0.1 #small prior to avoid overfitting\n",
    "    b_poverty = pm.Normal('b_poverty', mu = 0, sd = σ_prior)\n",
    "    b_l_population_density = pm.Normal('b_l_population_density', mu = 0, sd = σ_prior)\n",
    "    b_unemployment_rate = pm.Normal('b_unemployment_rate', mu = 0, sd = σ_prior)\n",
    "    \n",
    "    μ = constant + b_poverty*poverty_norm + b_l_population_density*l_population_density_norm + b_unemployment_rate*unemployment_rate_norm\n",
    "    σ = pm.HalfNormal('σ', 1)\n",
    "\n",
    "    arrest_rate = pm.Normal('arrest_rate', μ, σ, observed=arrest_rate_norm)\n",
    "    trace_normal = pm.sample(1500, tune=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54071cc9-516b-4440-b692-d6c85b402dc2",
   "metadata": {},
   "source": [
    "We now sample from the posterior predictive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd510fa-78f2-4ba8-93d9-a93c7635a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with normal:\n",
    "    ppc_normal = pm.sample_posterior_predictive(trace_normal, var_names = ['arrest_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3867e-0988-433c-8209-8924fa42236c",
   "metadata": {},
   "source": [
    "Having sampled from the posterior predictive, we now generate the trace plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d3341-535f-44c4-86e3-f4dc00e8abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_posterior_normal = az.from_pymc3(trace_normal, posterior_predictive = ppc_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc577ec1-6f34-444e-a272-0b65c714acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_arrest = ['b_poverty','b_l_population_density','b_unemployment_rate']\n",
    "az.summary(data_posterior_normal.posterior,variables_arrest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8ef0e-bca4-4ba5-967d-b87d17d5c85e",
   "metadata": {},
   "source": [
    "All r-hat values have a value of 1, which confirms that the sampling process was successful. Looking at the coefficient of interest, we can see that the mean of the coefficient of the posterior distribution for poverty is 0.16. This means that with a 1 standard deviation increase in poverty, given our data, we should expect a 0.16 standard deviation increase in the arrest rate. The 94% credibility interval for this variable shows us that the value of the coefficient lies between 0.138 and 0.187 with a 94% probability. Furthermore, we can see that both the unemployment rate and log of population density also have positive coefficients with 94% credibility intervals that are quite tight and all lie within positive values, as we expected beforehand.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de80767-bc2a-4539-9fe5-7fb68c08974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with normal:\n",
    "    pm.plot_trace(trace_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8792a4da-e726-4efe-b767-6ccff5fd7aeb",
   "metadata": {},
   "source": [
    "***\n",
    "### F.6.4 Results from the trace plots\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8e222b-6662-44e8-8629-22c69adf3d5f",
   "metadata": {},
   "source": [
    "When looking at the distributions, we can see that each posterior distribution is centered around a certain value: \n",
    "\n",
    "* The `b_poverty_rate` is around 0.16 \n",
    "* The `b_l_population_density` is around 0.23\n",
    "* The `b_unemployment_rate` is around 0.13\n",
    "\n",
    "More importantly, there are three features to look at when observing the trace plots:\n",
    "1. The plot should be stationary - not trending upwards or downwards\n",
    "2. Condensed zig-zagging of the trace \n",
    "3. Different chains covering same regions i.e. convergance\n",
    "\n",
    "In all of our trace plots all three features are satisfied, so that we can conclude that the algorithm has convereged and that we are seeing the true posterior distribution of the parameters in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570adc92-82f6-4af4-9f0f-addee9f7ff20",
   "metadata": {},
   "source": [
    "***\n",
    "### F.6.5 Checking model fit\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223aaa7e-039d-4920-8872-8ce8c1876999",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'y':arrest_rate_norm, 'x1':poverty_norm, 'x2':l_population_density_norm, 'x3':unemployment_rate_norm })\n",
    "#Model fit plot\n",
    "percentiles = np.percentile(ppc_normal['arrest_rate'],[2.5,97.5],axis=[0]).T\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(30, 15))\n",
    "\n",
    "ax1.vlines(df.x1,percentiles[:,0],percentiles[:,1],alpha=0.1)\n",
    "ax1.scatter(df.x1,df.y, color='r')\n",
    "ax1.set(xlabel='$poverty$', ylabel='$arrest rate$');\n",
    "\n",
    "ax2.vlines(df.x2,percentiles[:,0],percentiles[:,1],alpha=0.1)\n",
    "ax2.scatter(df.x2,df.y, color='r')\n",
    "ax2.set(xlabel='$population_density$', ylabel='$arrest rate$');\n",
    "\n",
    "ax3.vlines(df.x3,percentiles[:,0],percentiles[:,1],alpha=0.1)\n",
    "ax3.scatter(df.x3,df.y, color='r')\n",
    "ax3.set(xlabel='$unemployment_rate$', ylabel='$arrest rate$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc78c88-4efd-4834-8184-f6bc0fccd89f",
   "metadata": {},
   "source": [
    "The 95% confidence intevrals are shown by the vertical lines, while the dots are the observed values. Since most of our values are clustered closely, together it can be a bit demanding to see all observations and confidence intervals (we used alpha = 0.1 but the middle is still quite hard to observe). Overall, the model is doing good, since we can see that most of the observed values lie within the 95% confidence intervals. Obviously, there are a number of values that are outside the confidence intervals, but all together we are satisfied how the model fit our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88437a3-2e13-48b2-845a-e12b3344cd33",
   "metadata": {},
   "source": [
    "# G. Robustness analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c581397-7765-4261-9e15-b384530661ed",
   "metadata": {},
   "source": [
    "Even though we dealt with outliers in our dependent variables of interest, there are still a few outliers in our independent variables. These outliers could be driving the results of our findings. Furthermore, we believe our findings would be more robust if we could find a valid instrument for our independent variable of interest: `poverty rate`. Also, county-level data is not collected frequently. This problem makes it hard for us to add more years to our dataset and test if our results hold through time. Moreover, as noted in the introduction, we are not making any grandiose claims of causality since we acknowledge that we did not have a source of random exogenous variation in our independent variable of interest. Finally, one more step would be to collect data about police expenditure/probability of arrest at the county-level, which could be used in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c9b6c6-c596-4a09-b6ae-d3af44bc9e5f",
   "metadata": {},
   "source": [
    "# H. Discussion and conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab75f8d-3d60-4961-b387-0254a4a8176e",
   "metadata": {},
   "source": [
    "We find that poverty rates are related with arrest rates, and move in the same direction. With 94% probability we see that the the effect ranges between 0.13 to 0.18 standard deviations. Our assumption is that arrest rates are a good proxy for crime rates, and that poverty rates within a county affect crime rates. While analyzing our Bayesian model, we did not encounter problems. The sampling process was successful since all of the r-hat values had the value of 1. The trace plots also confirmed that the algorithm succesfuly converged and the model fit the data quite well.\n",
    "\n",
    "As mentioned earlier, the dataset we used was at the county level, and over two time periods. Our model results in low $R^2$ meaning that there is still a lot of variation in our dependent variable which is not explained by our independent variables. Furthermore, we are missing data on police funding, and more specifically, the probability of arrest, which could give us more insightful results. Lastly, we would like to use time-series data dating back to multiple years, instead of just two years. These addendums to this project would result in more robust findings.\n",
    "\n",
    "To conclude, we find that crime rates move in the same direction as poverty rates. This serves an important factor in mitigating crime-stricken areas. While a lot of other policies enacted to reduce crime, such as heavy police enforcement, it is also pertinent to view this issue through the lens of poverty. Lastly, it highlights the policy relevance of tackling poverty as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2c5bf-0413-453b-8a25-1f1ed6a99df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
